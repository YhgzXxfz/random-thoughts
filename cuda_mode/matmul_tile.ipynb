{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "dim3 = namedtuple(\"dim3\", [\"x\", \"y\", \"z\"], defaults=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim3(x=2, y=3, z=1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dim3(2, 3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x, d.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([256, 4]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "m1 = torch.randn((5120, 256))\n",
    "m2 = torch.randn((256, 5120))\n",
    "\n",
    "m1s = m1[:4]\n",
    "m2s = m2[:, :4]\n",
    "\n",
    "m1s.shape, m2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1c = m1.contiguous().cuda()\n",
    "m2c = m2.contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1sc = m1s.contiguous().cuda()\n",
    "m2sc = m2s.contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4065e+01,  6.8529e+00, -2.1297e+01,  9.0340e+00],\n",
       "        [ 1.3069e+00, -1.9484e+01, -6.0172e+00, -5.2774e+00],\n",
       "        [-5.4064e-03, -9.1612e+00,  9.6259e+00,  3.4818e+01],\n",
       "        [ 2.0240e+01, -8.6772e+00,  3.6141e+01, -1.3200e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1sc @ m2sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.5 ms ± 554 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit m1 @ m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.9 ms ± 559 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (m1c @ m2c).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.02 ms ± 13 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%timeit m1c @ m2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def matmul_2d_loop(\n",
    "    func: tp.Callable,\n",
    "    num_blocks: dim3,\n",
    "    threads_per_block: dim3,\n",
    "    *args,\n",
    "):\n",
    "    for i0 in range(num_blocks.x):\n",
    "        for i1 in range(num_blocks.y):\n",
    "            for j0 in range(threads_per_block.x):\n",
    "                for j1 in range(threads_per_block.y):\n",
    "                    func(dim3(i0, i1), dim3(j0, j1), threads_per_block, *args)\n",
    "\n",
    "\n",
    "def matmul_2d_kernel(\n",
    "    block_idx: dim3,\n",
    "    thread_idx: dim3,\n",
    "    block_dim: dim3,\n",
    "    A: torch.Tensor,\n",
    "    B: torch.Tensor,\n",
    "    out: torch.Tensor,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    k: int,\n",
    ") -> None:\n",
    "    row = block_idx.y * block_dim.y + thread_idx.y\n",
    "    col = block_idx.x * block_dim.x + thread_idx.x\n",
    "\n",
    "    if row >= height or col >= width:\n",
    "        return\n",
    "\n",
    "    o = 0.0\n",
    "    for i in range(k):\n",
    "        o += A[row * k + i] * B[i * width + col]\n",
    "    out[row * width + col] = o\n",
    "\n",
    "\n",
    "def matmul_2d(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    h, k = A.shape\n",
    "    k2, w = B.shape\n",
    "    assert k == k2, \"Size must match!\"\n",
    "\n",
    "    out = torch.zeros((h, w), dtype=A.dtype, device=A.device)\n",
    "    threads_per_block = dim3(16, 16)\n",
    "    num_blocks = dim3(math.ceil(w / threads_per_block.x), math.ceil(h / threads_per_block.y))\n",
    "    matmul_2d_loop(matmul_2d_kernel, num_blocks, threads_per_block, A.flatten(), B.flatten(), out.flatten(), h, w, k)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matmul_2d(m1s, m2s), m1s @ m2s, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Tiled Kernel with Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def matmul_2d_tiled_loop(\n",
    "    func: tp.Callable,\n",
    "    num_blocks: dim3,\n",
    "    threads_per_block: dim3,\n",
    "    shared_memory_size: int,\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    for i0 in range(num_blocks.x):\n",
    "        for i1 in range(num_blocks.y):\n",
    "            shared_memory = torch.zeros(shared_memory_size)\n",
    "            func(dim3(i0, i1), threads_per_block, shared_memory, *args, **kwargs)\n",
    "\n",
    "\n",
    "def matmul_2d_tiled_kernel(\n",
    "    block_idx: dim3,\n",
    "    block_dim: dim3,\n",
    "    shared_memory: torch.Tensor,\n",
    "    A: torch.Tensor,\n",
    "    B: torch.Tensor,\n",
    "    out: torch.Tensor,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    k: int,\n",
    "    tile_width: int,\n",
    ") -> None:\n",
    "    shared_memory_size = tile_width * tile_width\n",
    "    A_shared_memory, B_shared_memory = shared_memory[:shared_memory_size], shared_memory[shared_memory_size:]\n",
    "\n",
    "    for ph in range(int(math.ceil(k / tile_width))):\n",
    "\n",
    "        idx = ph * tile_width\n",
    "\n",
    "        # put data from corresponding parts of the two matrics into shared memory\n",
    "        for tile_row in range(block_dim.y):\n",
    "            for tile_col in range(block_dim.x):\n",
    "                row = block_idx.y * block_dim.y + tile_row\n",
    "                col = block_idx.x * block_dim.x + tile_col\n",
    "\n",
    "                A_shared_memory[tile_row * tile_width + tile_col] = (\n",
    "                    A[row * k + idx + tile_col] if row < height and idx + tile_col < k else 0.0\n",
    "                )\n",
    "                B_shared_memory[tile_row * tile_width + tile_col] = (\n",
    "                    B[(idx + tile_row) * width + col] if idx + tile_row < k and col < width else 0.0\n",
    "                )\n",
    "\n",
    "        # compute matmul for the data in tiles.\n",
    "        for tile_row in range(block_dim.y):\n",
    "            for tile_col in range(block_dim.x):\n",
    "                row = block_idx.y * block_dim.y + tile_row\n",
    "                col = block_idx.x * block_dim.x + tile_col\n",
    "\n",
    "                for i in range(tile_width):\n",
    "                    if row * width + col < len(out):\n",
    "                        out[row * width + col] += (\n",
    "                            A_shared_memory[tile_row * tile_width + i] * B_shared_memory[i * tile_width + tile_col]\n",
    "                        )\n",
    "\n",
    "\n",
    "def matmul_2d_tiled(A: torch.Tensor, B: torch.Tensor, tile_width: int) -> torch.Tensor:\n",
    "    h, k = A.shape\n",
    "    k2, w = B.shape\n",
    "    assert k == k2, \"Size must match!\"\n",
    "\n",
    "    out = torch.zeros((h, w), dtype=A.dtype, device=A.device)\n",
    "    threads_per_block = dim3(tile_width, tile_width)\n",
    "    num_blocks = dim3(math.ceil(w / threads_per_block.x), math.ceil(h / threads_per_block.y))\n",
    "    matmul_2d_tiled_loop(\n",
    "        matmul_2d_tiled_kernel,\n",
    "        num_blocks,\n",
    "        threads_per_block,\n",
    "        tile_width * tile_width * 2,  # tile_width ^2 for both matrices, thus * 2.\n",
    "        A.flatten(),\n",
    "        B.flatten(),\n",
    "        out.flatten(),\n",
    "        h,\n",
    "        w,\n",
    "        k,\n",
    "        tile_width,\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matmul_2d_tiled(m1s, m2s, tile_width=16), m1s @ m2s, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Tiled Kernel with Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from threading import Thread, Barrier\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    print(x)\n",
    "    print(-x)\n",
    "    print(x * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "-1\n",
      "10\n",
      "2\n",
      "-2\n",
      "20\n",
      "3\n",
      "-3\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "with ThreadPoolExecutor(num) as ex:\n",
    "    list(ex.map(lambda i: func(i), range(1, num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_b(x, b):\n",
    "    print(x)\n",
    "    b.wait()\n",
    "    print(-x)\n",
    "    b.wait()\n",
    "    print(x * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "-3\n",
      "-1\n",
      "-2\n",
      "30\n",
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "b = Barrier(num)\n",
    "with ThreadPoolExecutor(num) as ex:\n",
    "    list(ex.map(lambda i: func_b(i, b), range(1, num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import typing as tp\n",
    "from threading import Thread, Barrier\n",
    "\n",
    "\n",
    "def matmul_2d_tiled_with_threads_loop(\n",
    "    func: tp.Callable,\n",
    "    num_blocks: dim3,\n",
    "    threads_per_block: dim3,\n",
    "    shared_memory_size: int,\n",
    "    *args,\n",
    "    **kwargs,\n",
    "):\n",
    "    for i0 in range(num_blocks.x):\n",
    "        for i1 in range(num_blocks.y):\n",
    "            shared_memory = torch.zeros(shared_memory_size)\n",
    "            syncb = Barrier(threads_per_block.y * threads_per_block.x)\n",
    "            threads = [\n",
    "                Thread(\n",
    "                    target=func,\n",
    "                    args=(dim3(i0, i1), dim3(p, o), threads_per_block, shared_memory, syncb, *args),\n",
    "                    kwargs=kwargs,\n",
    "                )\n",
    "                for o in range(threads_per_block.y)\n",
    "                for p in range(threads_per_block.x)\n",
    "            ]\n",
    "            for thread in threads:\n",
    "                thread.start()\n",
    "\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "\n",
    "def matmul_2d_tiled_with_threads_kernel(\n",
    "    block_idx: dim3,\n",
    "    thread_idx: dim3,\n",
    "    block_dim: dim3,\n",
    "    shared_memory: torch.Tensor,\n",
    "    syncb: Barrier,\n",
    "    A: torch.Tensor,\n",
    "    B: torch.Tensor,\n",
    "    out: torch.Tensor,\n",
    "    height: int,\n",
    "    width: int,\n",
    "    k: int,\n",
    "    tile_width: int,\n",
    ") -> None:\n",
    "    shared_memory_size = tile_width * tile_width\n",
    "    A_shared_memory, B_shared_memory = shared_memory[:shared_memory_size], shared_memory[shared_memory_size:]\n",
    "\n",
    "    tile_row = thread_idx.y\n",
    "    tile_col = thread_idx.x\n",
    "    row = block_idx.y * block_dim.y + tile_row\n",
    "    col = block_idx.x * block_dim.x + tile_col\n",
    "\n",
    "    p = 0.0\n",
    "    for ph in range(int(math.ceil(k / tile_width))):\n",
    "\n",
    "        idx = ph * tile_width\n",
    "\n",
    "        # put data from corresponding parts of the two matrics into shared memory\n",
    "        A_shared_memory[tile_row * tile_width + tile_col] = (\n",
    "            A[row * k + idx + tile_col] if row < height and idx + tile_col < k else 0.0\n",
    "        )\n",
    "        B_shared_memory[tile_row * tile_width + tile_col] = (\n",
    "            B[(idx + tile_row) * width + col] if idx + tile_row < k and col < width else 0.0\n",
    "        )\n",
    "        syncb.wait()\n",
    "\n",
    "        # compute matmul for the data in tiles.\n",
    "        for i in range(tile_width):\n",
    "            p += A_shared_memory[tile_row * tile_width + i] * B_shared_memory[i * tile_width + tile_col]\n",
    "        syncb.wait()\n",
    "\n",
    "    # if row * width + col < len(out):\n",
    "    if row < height and col < width:\n",
    "        out[row * width + col] = p\n",
    "\n",
    "\n",
    "def matmul_2d_tiled_with_threads(A: torch.Tensor, B: torch.Tensor, tile_width: int) -> torch.Tensor:\n",
    "    h, k = A.shape\n",
    "    k2, w = B.shape\n",
    "    assert k == k2, \"Size must match!\"\n",
    "\n",
    "    out = torch.zeros((h, w), dtype=A.dtype, device=A.device)\n",
    "    threads_per_block = dim3(tile_width, tile_width)\n",
    "    num_blocks = dim3(math.ceil(w / threads_per_block.x), math.ceil(h / threads_per_block.y))\n",
    "    matmul_2d_tiled_with_threads_loop(\n",
    "        matmul_2d_tiled_with_threads_kernel,\n",
    "        num_blocks,\n",
    "        threads_per_block,\n",
    "        tile_width * tile_width * 2,  # tile_width ^2 for both matrices, thus * 2.\n",
    "        A.flatten(),\n",
    "        B.flatten(),\n",
    "        out.flatten(),\n",
    "        h,\n",
    "        w,\n",
    "        k,\n",
    "        tile_width,\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matmul_2d_tiled_with_threads(m1s, m2s, tile_width=16), m1s @ m2s, atol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernel Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cuda_source = Path(\"matmul_tiled_dynamic.cu\").read_text()\n",
    "cpp_source = \"torch::Tensor matmul_tiled(torch::Tensor A, torch::Tensor B);\"\n",
    "# You may need to check the line below\n",
    "os.environ[\"CUDA_HOME\"] = \"/public/apps/cuda/12.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/qingfeiyou/.conda/envs/cuda-mode/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1962: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "matmul_dynamic_module = load_inline(\n",
    "    name=\"matmul_tiled_dynamic\",\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"matmul_tiled\"],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    # build_directory='./cuda_build',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'matmul_tiled']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(matmul_dynamic_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4065e+01,  6.8529e+00, -2.1297e+01,  9.0340e+00],\n",
       "        [ 1.3069e+00, -1.9484e+01, -6.0172e+00, -5.2774e+00],\n",
       "        [-5.4064e-03, -9.1612e+00,  9.6259e+00,  3.4818e+01],\n",
       "        [ 2.0240e+01, -8.6772e+00,  3.6141e+01, -1.3200e+01]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matmul_dynamic_module.matmul_tiled(m1sc, m2sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matmul_dynamic_module.matmul_tiled(m1sc, m2sc).cpu(), m1s @ m2s, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3 ms ± 508 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit matmul_dynamic_module.matmul_tiled(m1c, m2c).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Kernel Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cuda_source = Path(\"matmul_tiled_static.cu\").read_text()\n",
    "cpp_source = \"torch::Tensor matmul_tiled(torch::Tensor A, torch::Tensor B);\"\n",
    "# You may need to check the line below\n",
    "os.environ[\"CUDA_HOME\"] = \"/public/apps/cuda/12.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "matmul_static_module = load_inline(\n",
    "    name=\"matmul_tiled_static\",\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"matmul_tiled\"],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    # build_directory='./cuda_build',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'matmul_tiled']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(matmul_static_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(matmul_static_module.matmul_tiled(m1sc, m2sc).cpu(), m1s @ m2s, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.1 ms ± 241 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit matmul_static_module.matmul_tiled(m1c, m2c).cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
