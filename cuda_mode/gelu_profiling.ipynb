{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Python Impl with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x: torch.Tensor):\n",
    "    \"\"\"\n",
    "    This implementation is slow on GPU because there are multiple data movement\n",
    "    between HBM and SRAM.\n",
    "\n",
    "    Every time there is an operation to the input \"x\", the input is loaded\n",
    "    from the global memory to SRAM, and the result is sent back to global memory\n",
    "    after computation is completed, back and forth.\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + torch.tanh((2 / torch.pi) ** 0.5 * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.randn((1024, 1024), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the results are the same.\n",
    "gelu(x) - F.gelu(x, approximate=\"tanh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this implementation is **5x slower** than PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 µs ± 97.9 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gelu(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 µs ± 235 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit F.gelu(x, approximate=\"tanh\"); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Fused Kernel with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "cuda_source = Path(\"gelu.cu\").read_text()\n",
    "cpp_source = \"torch::Tensor gelu(const torch::Tensor& input);\"\n",
    "# You may need to check the line below\n",
    "os.environ[\"CUDA_HOME\"] = \"/public/apps/cuda/12.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/home/qingfeiyou/.conda/envs/cuda-mode/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1962: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "module = load_inline(\n",
    "    name=\"gelu\",\n",
    "    cpp_sources=cpp_source,\n",
    "    cuda_sources=cuda_source,\n",
    "    functions=[\"gelu\"],\n",
    "    with_cuda=True,\n",
    "    extra_cuda_cflags=[\"-O2\"],\n",
    "    # build_directory='./cuda_build',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00, -1.4901e-08],\n",
       "        [ 0.0000e+00, -5.9605e-08,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        ...,\n",
       "        [ 0.0000e+00, -2.9802e-08, -5.2154e-08,  ...,  0.0000e+00,\n",
       "         -5.9605e-08,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00, -2.9802e-08,  ..., -4.4703e-08,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00, -5.9605e-08,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the results are the same.\n",
    "module.gelu(x) - F.gelu(x, approximate=\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3842e-07, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(module.gelu(x) - gelu(x)).abs().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see our custom kernel is close to PyTorch Implementation in terms of latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.3 µs ± 81.6 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit _ = module.gelu(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fused all computation around the input into a single kernel, to reduce the data movement between global memory and SRAM, thus reduced latency."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
