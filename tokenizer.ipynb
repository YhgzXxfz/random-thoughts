{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unicode Code Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"æ‹¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12354"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"ã‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"æ‚¨å¥½ï¼ðŸ˜Š hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24744, 22909, 65281, 128522, 32, 104, 101, 108, 108, 111]\n"
     ]
    }
   ],
   "source": [
    "print([ord(c) for c in example])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just using Unicode Code Point? => Unicode is not stable but keeps changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTF-8 / UTF-16 / UTF-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xe6\\x82\\xa8\\xe5\\xa5\\xbd\\xef\\xbc\\x81\\xf0\\x9f\\x98\\x8a hello'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# ASCII code points are encoded as they are, while non-english words and emojis are encoded into more bytes.\n",
    "# Therefore, the length of the encoded string is longer than the original one.\n",
    "print(len(example))\n",
    "print(len(example.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230, 130, 168, 229, 165, 189, 239, 188, 129, 240, 159, 152, 138, 32, 104, 101, 108, 108, 111]\n"
     ]
    }
   ],
   "source": [
    "print(list(example.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 168, 96, 125, 89, 1, 255, 61, 216, 10, 222, 32, 0, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Not efficient as there are one starting \"0\" for ASCII characters.\n",
    "\"\"\"\n",
    "print(list(example.encode(\"utf-16\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 0, 0, 168, 96, 0, 0, 125, 89, 0, 0, 1, 255, 0, 0, 10, 246, 1, 0, 32, 0, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Not efficient as there are many starting \"0\"s for ASCII characters.\n",
    "\"\"\"\n",
    "print(list(example.encode(\"utf-32\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not just using UTF-8 encoding? => It only has 256 vocab_size, resulting in limited ability to attend to long sequence, given a certain context length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aacbbbaadaaabbabdbdc\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "a = ['a', 'b', 'c', 'd']\n",
    "tiny_example = []\n",
    "\n",
    "for _ in range(20):\n",
    "    tiny_example.extend(random.sample(a, 1))\n",
    "\n",
    "tiny_example = ''.join(tiny_example)\n",
    "print(tiny_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for lead, follower in zip(tiny_example, tiny_example[1:]):\n",
    "    counts[(lead, follower)] = counts.get((lead, follower), 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('a', 'a'), 4), (('b', 'b'), 3), (('b', 'a'), 2), (('a', 'b'), 2), (('b', 'd'), 2), (('a', 'c'), 1), (('c', 'b'), 1), (('a', 'd'), 1), (('d', 'a'), 1), (('d', 'b'), 1), (('d', 'c'), 1)]\n"
     ]
    }
   ],
   "source": [
    "counts = sorted(counts.items(), key=lambda entry: entry[1], reverse=True)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('a', 'a'), 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(counts.items(), default=None, key=lambda entry: entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'a')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(counts, key=counts.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 99, 98, 98, 98, 65, 100, 65, 97, 98, 98, 97, 98, 100, 98, 100, 99]\n",
      "AcbbbAdAabbabdbdc\n"
     ]
    }
   ],
   "source": [
    "new_token_index = 65\n",
    "i = 0\n",
    "token_ids = []\n",
    "while i < len(tiny_example):\n",
    "    pair = max(counts, key=counts.get)\n",
    "    if i < len(tiny_example) - 1 and tiny_example[i] == pair[0] and tiny_example[i+1] == pair[1]:\n",
    "        # merge\n",
    "        token_ids.append(new_token_index)\n",
    "        i += 2\n",
    "    else:\n",
    "        token_ids.append(ord(tiny_example[i]))\n",
    "        i += 1\n",
    "\n",
    "print(token_ids)\n",
    "print(''.join([chr(token_id) for token_id in token_ids]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "import typing as tp\n",
    "\n",
    "def get_stats(token_ids: tp.List[int]) -> tp.Dict[tp.Tuple[int, int], int]:\n",
    "    counts = {}\n",
    "    for lead, follower in zip(token_ids, token_ids[1:]):\n",
    "        counts[(lead, follower)] = counts.get((lead, follower), 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge_with_pair(token_ids: tp.List[int], pair: tp.Tuple[int, int], new_token_index: int) -> tp.List[int]:\n",
    "    i = 0\n",
    "    new_token_ids = []\n",
    "    while i < len(token_ids):\n",
    "        if i < len(token_ids) - 1 and token_ids[i] == pair[0] and token_ids[i+1] == pair[1]:\n",
    "            # merge\n",
    "            new_token_ids.append(new_token_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_token_ids.append(token_ids[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_token_ids\n",
    "\n",
    "def merge(token_ids: tp.List[int], num_merges: int, start_index: int) -> tp.Tuple[tp.List[int], tp.Dict]:\n",
    "    new_token_mapping = {}\n",
    "    for i in range(num_merges):\n",
    "        stats = get_stats(token_ids)\n",
    "        pair = max(stats, key=stats.get)\n",
    "        new_token_index = start_index + i\n",
    "        print(f\"Merging {pair} to a new token {new_token_index}\")\n",
    "        token_ids = merge_with_pair(token_ids, pair, new_token_index)\n",
    "        new_token_mapping[pair] = new_token_index\n",
    "\n",
    "    return token_ids, new_token_mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token ids: 607\n",
      "Merging (101, 32) to a new token 256\n",
      "Merging (105, 110) to a new token 257\n",
      "Merging (99, 111) to a new token 258\n",
      "Merging (32, 97) to a new token 259\n",
      "Merging (105, 116) to a new token 260\n",
      "Merging (101, 114) to a new token 261\n",
      "Merging (97, 116) to a new token 262\n",
      "Merging (32, 116) to a new token 263\n",
      "Merging (226, 128) to a new token 264\n",
      "Merging (258, 109) to a new token 265\n",
      "After BPE: 510\n",
      "Compression rate: 1.19x\n"
     ]
    }
   ],
   "source": [
    "# Training data (ahaha)\n",
    "input = \"\"\"Now, Unicode does also include many â€œprecomposedâ€ code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 â€œÃâ€ latin capital letter a with acute or U+1EC7 â€œá»‡â€ latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they donâ€™t use dynamic composition that much in typical text.\"\"\"\n",
    "\n",
    "# Training\n",
    "ori_token_ids = list(map(int, input.encode(\"utf-8\")))\n",
    "print(f\"Original token ids: {len(ori_token_ids)}\")\n",
    "# new_token_mapping is the tokenizer that we trained.\n",
    "token_ids, new_token_mapping = merge(ori_token_ids, num_merges=10, start_index=256)\n",
    "\n",
    "print(f\"After BPE: {len(token_ids)}\")\n",
    "print(f\"Compression rate: {len(ori_token_ids) / len(token_ids):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (105, 110): 257,\n",
       " (99, 111): 258,\n",
       " (32, 97): 259,\n",
       " (105, 116): 260,\n",
       " (101, 114): 261,\n",
       " (97, 116): 262,\n",
       " (32, 116): 263,\n",
       " (226, 128): 264,\n",
       " (258, 109): 265}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_token_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n",
      "b'in'\n",
      "b'i'\n",
      "b'n'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "vocab is essential to decoding.\n",
    "\"\"\"\n",
    "vocab = {i: bytes([i]) for i in range(256)}\n",
    "for pair, new_token_index in new_token_mapping.items():\n",
    "    vocab[new_token_index] = vocab[pair[0]] + vocab[pair[1]] \n",
    "print(len(vocab))\n",
    "print(vocab[257])\n",
    "print(vocab[105])\n",
    "print(vocab[110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "\n",
    "def decode(token_ids: tp.List[int]) -> str:\n",
    "    tokens = b\"\".join(vocab[token_id] for token_id in token_ids)\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(token_ids) == example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 97 is ASCII character 'a'\n",
    "decode([97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[82], line 6\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(token_ids)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(token_ids: tp\u001b[38;5;241m.\u001b[39mList[\u001b[38;5;28mint\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(vocab[token_id] \u001b[38;5;28;01mfor\u001b[39;00m token_id \u001b[38;5;129;01min\u001b[39;00m token_ids)\n\u001b[0;32m----> 6\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# 128 does not comply with utf-8 format.\n",
    "decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï¿½'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import typing as tp\n",
    "\n",
    "\n",
    "def decode_with_fallback(token_ids: tp.List[int]) -> str:\n",
    "    tokens = b\"\".join(vocab[token_id] for token_id in token_ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "decode_with_fallback([128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "new_token_mapping is essential to encoding.\n",
    "\"\"\"\n",
    "import typing as tp\n",
    "\n",
    "def encode(input: str) -> tp.List[int]:\n",
    "    token_ids = list(map(int, input.encode(\"utf-8\")))\n",
    "    while len(token_ids) > 1: # stats will be empty if len(token_ids) == 1.\n",
    "        stats = get_stats(token_ids)\n",
    "        # Get the mapping, start from the smallest new token id, merge the token id list repeatedly.\n",
    "        pair = min(stats, key=lambda p: new_token_mapping.get(p, float('inf')))\n",
    "        if pair not in new_token_mapping:\n",
    "            # means no pair can be found in the mapping, nothing to be merged.\n",
    "            break\n",
    "        token_ids = merge_with_pair(token_ids, pair, new_token_mapping[pair])\n",
    "    return token_ids\n",
    "\n",
    "encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Single Character\n",
    "encode(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world!'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ASCII data (subset of training data)\n",
    "decode(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data\n",
    "decode(encode(example)) == example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picked from https://en.wikipedia.org/wiki/Emoji.\n",
    "validation_data = \"\"\"Originally meaning pictograph, the word emoji comes from Japanese e (çµµ, 'picture') + moji (æ–‡å­—, 'character'); the resemblance to the English words emotion and emoticon is purely coincidental.[4] The first emoji set was created by Japanese phone carrier SoftBank in 1997,[5] with emoji becoming increasingly popular worldwide in the 2010s after Unicode began encoding emoji into the Unicode Standard.[6][7][8] They are now considered to be a large part of popular culture in the West and around the world.[9][10] In 2015, Oxford Dictionaries named the Face with Tears of Joy emoji (ðŸ˜‚) the word of the year.[11][12]\"\"\"\n",
    "decode(encode(validation_data)) == validation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT2 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We see the regular expression below split a sequence into pieces and concatenate them into a list of strings.\n",
    "\n",
    "Why do that? Consider this example in GPT2 paper: \"dog\", \"dog!\", \"dog.\", \"dog?\".\n",
    "Without the regex, \"d\" and \"o\" are likely to be merged, then merged with \"g\", then with punctuation.\n",
    "That is not what we want, as we do not want to merge semantics with punctuation.\n",
    "\"\"\"\n",
    "# regex is an extension of python re package.\n",
    "import regex\n",
    "\n",
    "# https://github.com/openai/gpt-2/blob/master/src/encoder.py#L53\n",
    "gpt2pat = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world', ' how', ' are', ' you']\n",
      "['Hello', ' world', '123', ' how', ' are', ' you']\n",
      "['Hello', \"'ve\", ' world', ' how', ' are', ' you']\n",
      "['Hello', ' world', ' how', ' are', ' you', '!!!']\n",
      "['Hello', ' world', ' how', ' are', '    ', ' you']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It matches a subsequence of letters followed by an optional space ( ?\\p{L}+), or a series of numbers followed by an optional space ( ?\\p{N}+).\n",
    "In this way, it avoids BPE to merge a letter / number followed by a space.\n",
    "There are other cases in this regex.\n",
    "\"\"\"\n",
    "# Letters\n",
    "print(gpt2pat.findall(\"Hello world how are you\"))\n",
    "\n",
    "# Numbers\n",
    "print(gpt2pat.findall(\"Hello world123 how are you\"))\n",
    "\n",
    "# Apostrophy\n",
    "print(gpt2pat.findall(\"Hello've world how are you\"))\n",
    "\n",
    "# Punctuation\n",
    "print(gpt2pat.findall(\"Hello world how are you!!!\"))\n",
    "\n",
    "# Extra spaces (always allow the last space to be with the next non-space token)\n",
    "print(gpt2pat.findall(\"Hello world how are     you\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', \"'\", 'VE', ' world', ' how', ' are', ' you']\n",
      "['Hello', '`', 've', ' world', ' how', ' are', ' you']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Also note that the regex above also does not cover certain cases.\n",
    "\"\"\"\n",
    "# 'VE is not recognized and chunked together\n",
    "print(gpt2pat.findall(\"Hello'VE world how are you\"))\n",
    "\n",
    "# `ve is not recognized as 've\n",
    "print(gpt2pat.findall(\"Hello`ve world how are you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 18435, 995, 10185]\n",
      "[262, 22691, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT2 tokenizer. Vocab size is ~50k.\n",
    "encoder = tiktoken.get_encoding(\"gpt2\") # this will download gpt2 tokenizer.\n",
    "print(encoder.encode(\"    Hello world!!!\"))\n",
    "\n",
    "# GPT4 tokenizer. Vocab size is ~100k. Tackled cases with apostrophy. Added additional treatments.\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\") # this will download gpt4 tokenizer.\n",
    "print(encoder.encode(\"    Hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not use a very large vocab_size?\n",
    "\n",
    "1) Large vocab_size leads to some / many tokens under-trained since we cannot guarantee a sufficient train data for tokenizer.\n",
    "2) Huge vocab_size leads to a lot of merging, and giant blocks of tokens being merged into one, which reduces LLM's flexibility.\n",
    "3) Computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# How to retrain tokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize the vocab_size in embedding layer and the final linear layer, freeze the base language model, and train the model.\n",
    "\n",
    "Note only the embedding layer and linear layer is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM's Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of letters in a \"word\"\n",
    "\n",
    "\".DefaultCellStyle\" is one single token in GPT4 tokenizer. The model fails to count the \"l\"s in in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse String\n",
    "\n",
    "\".DefaultCellStyle\" is one single token in GPT4 tokenizer. The model fails to reverse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreign Languages\n",
    "\n",
    "Larger vocabulary in Unicode (Chinese / Korean vocab size is 10x than letter-based vocab: https://www.reedbeta.com/blog/programmers-intro-to-unicode/#scripts)\n",
    "\n",
    "1) Fewer data of foreign languages than of English.\n",
    "2) Harder to find sufficient training data for the tokenizer. (to have enough merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math\n",
    "\n",
    "Really depends on how the numbers are merged by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT2 Not Good in Python\n",
    "\n",
    "Spaces (indentation) is not well handled in GPT2 tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tokens \"<|endoftext|>\"\n",
    "\n",
    "GPT4 recognize it as a single token, and will not see it as a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trailing Whitespace\n",
    "\n",
    "Tokenizer always merges a starting whitespace with other tokens. Rarely does it merge a series of tokens and add a whitespace at the end.\n",
    "\n",
    "Therefore, the trailing whitespace is an isolated token at inference time, while the model has seen very little this example in training.\n",
    "\n",
    "Thus LLM gives poor results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstable Tokens (Partial Tokens)\n",
    "\n",
    "\"`.DefaultCellSty`\" appears rarely in the training set. Thus the model would output directly an end of text token when it sees it prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit User Name\n",
    "\n",
    "Tokenizer dataset != Training dataset.\n",
    "\n",
    "Reddit User names probably appeared in the tokenizer dataset a lot times and got merged to have a dedicated token.\n",
    "While they did not appear in the training dataset, and thus those specific tokens never got (well) trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON vs YAML\n",
    "\n",
    "YAML shorter than JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# minBPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training code of tiktoken (OpenAI did not release the training code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) `tiktoken` would be a go-to solution if not retraining tokenizer.\n",
    "2) `sentencepiece` with BPE can be used to train a tokenizer. Be carefully with fallback, and tons of configs, and normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
