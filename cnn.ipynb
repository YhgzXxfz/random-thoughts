{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11f39fd10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1337)\n",
    "input_image = np.random.randn(5, 7).astype(np.float32)\n",
    "kernel = np.random.randn(3, 3).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.7359321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(input_image[0:3, 0:3] * kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7359,  1.6481,  0.5012, -5.2333, -0.5101],\n",
       "          [-1.5167,  3.7987,  0.8453, -3.0083, -2.9609],\n",
       "          [-1.3546, -0.2310,  2.4693, -1.8407, -5.2685]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_tensor = torch.tensor(\n",
    "    np.expand_dims(np.expand_dims(input_image, 0), 0), dtype=torch.float32, requires_grad=True\n",
    ")\n",
    "kernel_tensor = torch.tensor(np.expand_dims(np.expand_dims(kernel, 0), 0), dtype=torch.float32, requires_grad=True)\n",
    "output_tensor = F.conv2d(input_image_tensor, kernel_tensor)\n",
    "output_tensor.retain_grad()\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, kernel):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    input_height, input_weight = input.shape\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    output_height = input_height - kernel_height + 1\n",
    "    output_weight = input_weight - kernel_weight + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            output[i, j] = np.sum(input[i : i + kernel_height, j : j + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(conv2d(input_image, kernel), output_tensor.data.squeeze(0).squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-14.3974, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.sum(output_tensor)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1., 1.]]]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.533592963478067"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(input_image[0:3, 0:5] * output_tensor.grad.squeeze(0).squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.5336, -5.6544, -5.7220],\n",
       "          [ 2.2203, -1.4431, -2.8828],\n",
       "          [ 2.0596, -1.3358, -1.9492]]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53359306, -5.65440559, -5.72200251],\n",
       "       [ 2.2202642 , -1.44307494, -2.88277555],\n",
       "       [ 2.05961275, -1.33583522, -1.94924402]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradients of conv is also a conv\n",
    "conv2d(input_image, kernel=output_tensor.grad.squeeze(0).squeeze(0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5335932, -5.6544065, -5.7220025],\n",
       "       [ 2.220264 , -1.4430748, -2.8827758],\n",
       "       [ 2.0596128, -1.3358345, -1.9492438]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_tensor.grad.squeeze(0).squeeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel=output_tensor.grad.squeeze(0).squeeze(0).numpy()),\n",
    "    kernel_tensor.grad.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7359,  0.5012, -0.5101],\n",
       "          [-1.3546,  2.4693, -5.2685]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(input_image_tensor, kernel_tensor, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def conv2d(input, kernel, stride: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    if not stride:\n",
    "        stride_height = stride_weight = 1\n",
    "    elif isinstance(stride, int):\n",
    "        stride_height = stride_weight = stride\n",
    "    else:\n",
    "        assert len(stride) == 2\n",
    "        stride_height, stride_weight = stride\n",
    "\n",
    "    input_height, input_weight = input.shape\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    # Adjust output size based on strides\n",
    "    output_height = int(np.floor((input_height - kernel_height) / stride_height)) + 1\n",
    "    output_weight = int(np.floor((input_weight - kernel_weight) / stride_weight)) + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            # For stride, the only difference is the starting point.\n",
    "            row = i * stride_height\n",
    "            col = j * stride_weight\n",
    "            output[i, j] = np.sum(input[row : row + kernel_height, col : col + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default is no stride\n",
    "np.allclose(\n",
    "    conv2d(input_image, kernel),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=1).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=2),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=2).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(1, 3)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=(1, 3)).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extreme Cases\n",
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=5),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=5).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4119, -3.3077, -1.5374, -0.9443, -3.7461, -0.1400, -0.9590],\n",
       "          [-1.4036, -1.7359,  1.6481,  0.5012, -5.2333, -0.5101,  2.8364],\n",
       "          [-0.2356, -1.5167,  3.7987,  0.8453, -3.0083, -2.9609,  2.5304],\n",
       "          [-2.5582, -1.3546, -0.2310,  2.4693, -1.8407, -5.2685, -1.3589],\n",
       "          [-1.6277,  0.9669,  0.7669,  1.8226, -0.4189, -2.9021, -0.7838]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(input_image_tensor, kernel_tensor, stride=1, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 2, 3, 0],\n",
       "       [0, 4, 5, 6, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "np.pad(a, ((2,), (1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, kernel, padding: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    if not padding:\n",
    "        padding_height = padding_weight = 0\n",
    "    elif isinstance(padding, int):\n",
    "        padding_height = padding_weight = padding\n",
    "    else:\n",
    "        assert len(padding) == 2\n",
    "        padding_height, padding_weight = padding\n",
    "\n",
    "    # Pad input\n",
    "    input = np.pad(input, ((padding_height,), (padding_weight,)), mode=\"constant\", constant_values=(0, 0))\n",
    "\n",
    "    input_height, input_weight = input.shape\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    output_height = input_height - kernel_height + 1\n",
    "    output_weight = input_weight - kernel_weight + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            output[i, j] = np.sum(input[i : i + kernel_height, j : j + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=1),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=1).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=(2, 1)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=(2, 1)).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extreme cases\n",
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=4),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=4).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding + Stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    input,\n",
    "    kernel,\n",
    "    stride: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "    padding: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    # Stride\n",
    "    if not stride:\n",
    "        stride_height = stride_weight = 1\n",
    "    elif isinstance(stride, int):\n",
    "        stride_height = stride_weight = stride\n",
    "    else:\n",
    "        assert len(stride) == 2\n",
    "        stride_height, stride_weight = stride\n",
    "\n",
    "    # Pad input\n",
    "    if not padding:\n",
    "        padding_height = padding_weight = 0\n",
    "    elif isinstance(padding, int):\n",
    "        padding_height = padding_weight = padding\n",
    "    else:\n",
    "        assert len(padding) == 2\n",
    "        padding_height, padding_weight = padding\n",
    "\n",
    "    input = np.pad(input, ((padding_height,), (padding_weight,)), mode=\"constant\", constant_values=(0, 0))\n",
    "\n",
    "    input_height, input_weight = input.shape\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    # Output size\n",
    "    output_height = int(np.floor((input_height - kernel_height) / stride_height)) + 1\n",
    "    output_weight = int(np.floor((input_weight - kernel_weight) / stride_weight)) + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            row = i * stride_height\n",
    "            col = j * stride_weight\n",
    "            output[i, j] = np.sum(input[row : row + kernel_height, col : col + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=2, padding=1),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=2, padding=1).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(1, 3), padding=(2, 1)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=(1, 3), padding=(2, 1)).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    input,\n",
    "    kernel,\n",
    "    stride: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "    padding: tp.Optional[tp.Union[int, tp.Tuple[int, int], str]] = None,\n",
    "):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    # Stride\n",
    "    if not stride:\n",
    "        stride_height = stride_weight = 1\n",
    "    elif isinstance(stride, int):\n",
    "        stride_height = stride_weight = stride\n",
    "    else:\n",
    "        assert len(stride) == 2\n",
    "        stride_height, stride_weight = stride\n",
    "\n",
    "    # Pad input\n",
    "    # Use 4 parameters for padding to deal with imbalanced cases.\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    if not padding:\n",
    "        padding_up = padding_down = padding_left = padding_right = 0\n",
    "    elif isinstance(padding, str):\n",
    "        if padding == \"valid\":\n",
    "            padding_up = padding_down = padding_left = padding_right = 0\n",
    "        elif padding == \"same\":\n",
    "            assert stride_height == 1 and stride_weight == 1, \"'same' padding can only be applied to stride == 1.\"\n",
    "            # new_input_height == input_height + 2 * pad\n",
    "            # output_height = new_input_height - kernel_height + 1\n",
    "            # We want output_height == input_height\n",
    "            # Thus, new_input_height - kernel_height + 1 == new_input_height - 2 * pad\n",
    "            # 2 * pad == kernel_height - 1, w.r.t pad is integer.\n",
    "            # In case of imbalance, we pad more in 'down' and 'right'.\n",
    "            padding_up = int(np.floor((kernel_height - 1) / 2))\n",
    "            padding_down = int(np.ceil((kernel_height - 1) / 2))\n",
    "            padding_left = int(np.floor((kernel_weight - 1) / 2))\n",
    "            padding_right = int(np.ceil((kernel_weight - 1) / 2))\n",
    "\n",
    "        elif padding == \"full\":\n",
    "            padding_up = padding_down = kernel_height - 1\n",
    "            padding_left = padding_right = kernel_weight - 1\n",
    "        else:\n",
    "            raise Exception(f\"{padding} is not recognized. Can only be 'valid' or 'same' or 'full'.\")\n",
    "\n",
    "    elif isinstance(padding, int):\n",
    "        padding_up = padding_down = padding_left = padding_right = padding\n",
    "    else:\n",
    "        assert len(padding) == 2\n",
    "        padding_up = padding_down = padding[0]\n",
    "        padding_left = padding_right = padding[1]\n",
    "\n",
    "    input = np.pad(\n",
    "        input, ((padding_up, padding_down), (padding_left, padding_right)), mode=\"constant\", constant_values=(0, 0)\n",
    "    )\n",
    "    input_height, input_weight = input.shape\n",
    "\n",
    "    output_height = int(np.floor((input_height - kernel_height) / stride_height)) + 1\n",
    "    output_weight = int(np.floor((input_weight - kernel_weight) / stride_weight)) + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            row = i * stride_height\n",
    "            col = j * stride_weight\n",
    "            output[i, j] = np.sum(input[row : row + kernel_height, col : col + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=2, padding=\"valid\"),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=2, padding=\"valid\").data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=1, padding=\"same\"),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=1, padding=\"same\").data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=1, padding=\"full\"),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=1, padding=(kernel.shape[0] - 1, kernel.shape[1] - 1))\n",
    "    .data.squeeze(0)\n",
    "    .squeeze(0)\n",
    "    .numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=2, padding=\"full\"),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=2, padding=(kernel.shape[0] - 1, kernel.shape[1] - 1))\n",
    "    .data.squeeze(0)\n",
    "    .squeeze(0)\n",
    "    .numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00874653,  0.3670187 ,  1.1185547 ],\n",
       "       [-0.00838993,  0.4663154 ,  1.2632687 ],\n",
       "       [-0.90165466, -1.0288427 ,  0.5696784 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5725032"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(input_image[:5:2, :5:2] * kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5725, -3.7247, -4.1936]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(input_image_tensor, kernel_tensor, dilation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (5 x 7). Kernel size: (11 x 11). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (5 x 7). Kernel size: (11 x 11). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "F.conv2d(input_image_tensor, kernel_tensor, dilation=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00874653,  0.        ,  0.3670187 ,  0.        ,  1.11855471],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.00838993,  0.        ,  0.46631539,  0.        ,  1.26326871],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-0.90165466,  0.        , -1.02884269,  0.        ,  0.56967843]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilation_height = dilation_weight = 2\n",
    "kernel_height, kernel_weight = kernel.shape\n",
    "dilated_kernel = np.zeros(\n",
    "    (\n",
    "        kernel_height + (dilation_height - 1) * (kernel_height - 1),\n",
    "        kernel_weight + (dilation_weight - 1) * (kernel_weight - 1),\n",
    "    )\n",
    ")\n",
    "for i in range(0, kernel_height):\n",
    "    for j in range(0, kernel_weight):\n",
    "        dilated_kernel[i * dilation_height, j * dilation_weight] = kernel[i, j]\n",
    "\n",
    "dilated_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "\n",
    "\n",
    "def conv2d(input, kernel, dilation: tp.Optional[tp.Union[int, tp.Tuple[int, int]]]):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    if not dilation:\n",
    "        dilation_height = dilation_weight = 1\n",
    "    elif isinstance(dilation, int):\n",
    "        dilation_height = dilation_weight = dilation\n",
    "    else:\n",
    "        assert len(dilation) == 2\n",
    "        dilation_height, dilation_weight = dilation\n",
    "\n",
    "    # Pad kernel\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "    dilated_kernel_height = kernel_height + (dilation_height - 1) * (kernel_height - 1)\n",
    "    dilated_kernel_weight = kernel_weight + (dilation_weight - 1) * (kernel_weight - 1)\n",
    "    dilated_kernel = np.zeros((dilated_kernel_height, dilated_kernel_weight))\n",
    "    for i in range(0, kernel_height):\n",
    "        for j in range(0, kernel_weight):\n",
    "            dilated_kernel[i * dilation_height, j * dilation_weight] = kernel[i, j]\n",
    "\n",
    "    input_height, input_weight = input.shape\n",
    "\n",
    "    output_height = input_height - dilated_kernel_height + 1\n",
    "    output_weight = input_weight - dilated_kernel_weight + 1\n",
    "    if output_height < 1 or output_weight < 1:\n",
    "        raise Exception(\"Kernel size cannot be larger then the input size.\")\n",
    "\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            output[i, j] = np.sum(input[i : i + dilated_kernel_height, j : j + dilated_kernel_weight] * dilated_kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, dilation=2),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, dilation=2).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, dilation=(1, 3)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, dilation=(1, 3)).data.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride + Padding + Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.56798711,  0.        ,  0.        ,  0.30438786],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [-1.0000296 ,  0.        ,  0.        , -2.45641783]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dilate(kernel, dilation):\n",
    "    if not dilation:\n",
    "        dilation_height = dilation_weight = 1\n",
    "    elif isinstance(dilation, int):\n",
    "        dilation_height = dilation_weight = dilation\n",
    "    else:\n",
    "        assert len(dilation) == 2\n",
    "        dilation_height, dilation_weight = dilation\n",
    "\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "    dilated_kernel = np.zeros(\n",
    "        (\n",
    "            kernel_height + (dilation_height - 1) * (kernel_height - 1),\n",
    "            kernel_weight + (dilation_weight - 1) * (kernel_weight - 1),\n",
    "        )\n",
    "    )\n",
    "    for i in range(0, kernel_height):\n",
    "        for j in range(0, kernel_weight):\n",
    "            dilated_kernel[i * dilation_height, j * dilation_weight] = kernel[i, j]\n",
    "\n",
    "    return dilated_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing as tp\n",
    "\n",
    "\n",
    "def get_strides(stride: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None):\n",
    "    if not stride:\n",
    "        stride_height = stride_weight = 1\n",
    "    elif isinstance(stride, int):\n",
    "        stride_height = stride_weight = stride\n",
    "    else:\n",
    "        assert len(stride) == 2\n",
    "        stride_height, stride_weight = stride\n",
    "    return stride_height, stride_weight\n",
    "\n",
    "\n",
    "def dilate(\n",
    "    kernel,\n",
    "    dilation: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "):\n",
    "    if not dilation:\n",
    "        dilation_height = dilation_weight = 1\n",
    "    elif isinstance(dilation, int):\n",
    "        dilation_height = dilation_weight = dilation\n",
    "    else:\n",
    "        assert len(dilation) == 2\n",
    "        dilation_height, dilation_weight = dilation\n",
    "\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "    dilated_kernel = np.zeros(\n",
    "        (\n",
    "            kernel_height + (dilation_height - 1) * (kernel_height - 1),\n",
    "            kernel_weight + (dilation_weight - 1) * (kernel_weight - 1),\n",
    "        )\n",
    "    )\n",
    "    for i in range(0, kernel_height):\n",
    "        for j in range(0, kernel_weight):\n",
    "            dilated_kernel[i * dilation_height, j * dilation_weight] = kernel[i, j]\n",
    "\n",
    "    return dilated_kernel\n",
    "\n",
    "\n",
    "def pad(\n",
    "    input,\n",
    "    kernel_height: int,\n",
    "    kernel_weight: int,\n",
    "    stride_height: int,\n",
    "    stride_weight: int,\n",
    "    padding: tp.Optional[tp.Union[int, tp.Tuple[int, int], str]] = None,\n",
    "):\n",
    "    if not padding:\n",
    "        padding_up = padding_down = padding_left = padding_right = 0\n",
    "    elif isinstance(padding, str):\n",
    "        if padding == \"valid\":\n",
    "            padding_up = padding_down = padding_left = padding_right = 0\n",
    "        elif padding == \"same\":\n",
    "            assert stride_height == 1 and stride_weight == 1, \"'same' padding can only be applied to stride == 1.\"\n",
    "            # new_input_height == input_height + 2 * pad\n",
    "            # output_height = new_input_height - kernel_height + 1\n",
    "            # We want output_height == input_height\n",
    "            # Thus, new_input_height - kernel_height + 1 == new_input_height - 2 * pad\n",
    "            # 2 * pad == kernel_height - 1, w.r.t pad is integer.\n",
    "            # In case of imbalance, we pad more in 'down' and 'right'.\n",
    "            padding_up = int(np.floor((kernel_height - 1) / 2))\n",
    "            padding_down = int(np.ceil((kernel_height - 1) / 2))\n",
    "            padding_left = int(np.floor((kernel_weight - 1) / 2))\n",
    "            padding_right = int(np.ceil((kernel_weight - 1) / 2))\n",
    "\n",
    "        elif padding == \"full\":\n",
    "            padding_up = padding_down = kernel_height - 1\n",
    "            padding_left = padding_right = kernel_weight - 1\n",
    "        else:\n",
    "            raise Exception(f\"{padding} is not recognized. Can only be 'valid' or 'same' or 'full'.\")\n",
    "\n",
    "    elif isinstance(padding, int):\n",
    "        padding_up = padding_down = padding_left = padding_right = padding\n",
    "    else:\n",
    "        assert len(padding) == 2\n",
    "        padding_up = padding_down = padding[0]\n",
    "        padding_left = padding_right = padding[1]\n",
    "\n",
    "    padded_input = np.pad(\n",
    "        input, ((padding_up, padding_down), (padding_left, padding_right)), mode=\"constant\", constant_values=(0, 0)\n",
    "    )\n",
    "    return padded_input\n",
    "\n",
    "\n",
    "def conv2d(\n",
    "    input,\n",
    "    kernel,\n",
    "    stride: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "    padding: tp.Optional[tp.Union[int, tp.Tuple[int, int], str]] = None,\n",
    "    dilation: tp.Optional[tp.Union[int, tp.Tuple[int, int]]] = None,\n",
    "):\n",
    "    assert len(input.shape) == 2\n",
    "    assert len(kernel.shape) == 2\n",
    "\n",
    "    # Stride\n",
    "    stride_height, stride_weight = get_strides(stride)\n",
    "\n",
    "    # Dilate Kernel\n",
    "    kernel = dilate(kernel, dilation)\n",
    "    kernel_height, kernel_weight = kernel.shape\n",
    "\n",
    "    # Pad Input\n",
    "    input = pad(input, kernel_height, kernel_weight, stride_height, stride_weight, padding)\n",
    "    input_height, input_weight = input.shape\n",
    "\n",
    "    # Output\n",
    "    output_height = int(np.floor((input_height - kernel_height) / stride_height)) + 1\n",
    "    output_weight = int(np.floor((input_weight - kernel_weight) / stride_weight)) + 1\n",
    "    output = np.zeros((output_height, output_weight))\n",
    "\n",
    "    for i in range(0, output_height):\n",
    "        for j in range(0, output_weight):\n",
    "            row = i * stride_height\n",
    "            col = j * stride_weight\n",
    "            output[i, j] = np.sum(input[row : row + kernel_height, col : col + kernel_weight] * kernel)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor).data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(2, 3)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=(2, 3)).data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(2, 3), padding=(1, 2)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=(2, 3), padding=(1, 2)).data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(2, 3), padding=(1, 2), dilation=(1, 3)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, stride=(2, 3), padding=(1, 2), dilation=(1, 3))\n",
    "    .data.squeeze(0)\n",
    "    .squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=\"same\"),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=\"same\").data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=\"same\", dilation=2),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=\"same\", dilation=2).data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, padding=\"same\", dilation=(2, 3)),\n",
    "    F.conv2d(input_image_tensor, kernel_tensor, padding=\"same\", dilation=(2, 3)).data.squeeze(0).squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, kernel, stride=(2, 3), padding=\"full\"),\n",
    "    F.conv2d(\n",
    "        input_image_tensor,\n",
    "        kernel_tensor,\n",
    "        stride=(2, 3),\n",
    "        padding=(kernel_tensor.shape[2] - 1, kernel_tensor.shape[3] - 1),\n",
    "    )\n",
    "    .data.squeeze(0)\n",
    "    .squeeze(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Prop with Stride / Padding / Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.7203, -1.0085],\n",
       "          [ 2.1243,  1.4154],\n",
       "          [ 1.3350,  0.6755]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor = F.conv2d(input_image_tensor, kernel_tensor, stride=(2, 3), padding=(1, 2), dilation=(1, 3))\n",
    "output_tensor.retain_grad()\n",
    "print(output_tensor.shape)\n",
    "output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_tensor.grad = None\n",
    "input_image_tensor.grad = None\n",
    "loss = torch.sum(output_tensor)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1.],\n",
       "          [1., 1.],\n",
       "          [1., 1.]]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2342,  0.0754, -0.1588],\n",
       "          [-2.1738,  0.3401,  2.5139],\n",
       "          [ 0.2342,  0.0754, -0.1588]]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_tensor.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.23418331,  0.07538714, -0.15879618],\n",
       "       [-2.17378468,  0.3401105 ,  2.51389518],\n",
       "       [ 0.23418331,  0.07538714, -0.15879618]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_grad = output_tensor.grad.squeeze(0).squeeze(0).numpy()\n",
    "# Gradients of conv is also a conv\n",
    "# stride in forward pass => dilation in backward pass\n",
    "# dilation in forward pass => stride in backward pass\n",
    "# padding in forward pass => padding in backward pass\n",
    "conv2d(input_image, output_grad, stride=(1, 3), padding=(1, 2), dilation=(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(\n",
    "    conv2d(input_image, output_grad, stride=(1, 3), padding=(1, 2), dilation=(2, 3)),\n",
    "    kernel_tensor.grad.squeeze(0).squeeze(0).numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Color\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "light_blue = [0.6, 0.8, 1.0]  # RGB values for light blue\n",
    "cmap = ListedColormap([light_blue])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(2, 2)\n",
    "im = ax.imshow(matrix, cmap=cmap)\n",
    "ax.axhline(0.5, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.axhline(1.5, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.axvline(0.5, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.axvline(1.5, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        plt.text(j, i, str(matrix[i, j]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karpathy-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
